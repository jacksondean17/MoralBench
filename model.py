from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.rate_limiters import InMemoryRateLimiter

class Model:
    def __init__(self, model_name="gpt-4"):
        self.model_name = model_name
        
        # Configure rate limiters based on model
        if model_name in ["gpt-4", "gpt-3.5-turbo"]:
            rate_limiter = InMemoryRateLimiter(
                requests_per_second=3,  # 3 requests per second
                check_every_n_seconds=0.1,
                max_bucket_size=10
            )
            self.client = ChatOpenAI(
                model_name=model_name,
                rate_limiter=rate_limiter
            )
        elif model_name == "sonnet-3.5":
            rate_limiter = InMemoryRateLimiter(
                requests_per_second=10,  # 10 request per second
                check_every_n_seconds=0.1,
                max_bucket_size=5
            )
            self.client = ChatAnthropic(
                model_name="claude-3-5-sonnet-20241022",
                rate_limiter=rate_limiter
            )
        elif model_name == "gemini":
            rate_limiter = InMemoryRateLimiter(
                requests_per_second=0.2,  # 15 requests per minute
                check_every_n_seconds=0.1,
                max_bucket_size=20
            )
            self.client = ChatGoogleGenerativeAI(
                model="gemini-1.5-flash",
                rate_limiter=rate_limiter
            )

    def read_prompt(self, file_path):
        """
        Reads the content of a prompt file.
        
        Args:
            file_path (str): Path to the prompt file.
            
        Returns:
            str: Content of the prompt file.
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read().strip()
        except FileNotFoundError:
            raise FileNotFoundError(f"Prompt file not found at: {file_path}")

    def generate_response(self, target_folder, question):
        """
        Generates a response using LangChain chat models.
        
        Args:
            target_folder (str): The folder containing the question file.
            question (str): The name of the question file (without extension).
            
        Returns:
            str: The response generated by the language model.
        """
        system_prompt = self.read_prompt('./template/moral_system.txt')
        user_prompt = self.read_prompt(f'./questions/{target_folder}/{question}.txt')
        
        print("The current question is:\n", user_prompt)

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        parser = StrOutputParser()
        response = self.client.invoke(messages)
        return parser.invoke(response)